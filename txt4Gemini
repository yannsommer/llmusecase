{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10334015,"sourceType":"datasetVersion","datasetId":6398729}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U -q 'google-generativeai>=0.8.3'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install langchain","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U langchain-community\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install unstructured","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install faiss-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List, Dict, Any\nimport os\nfrom pathlib import Path\nimport google.generativeai as genai\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nimport numpy as np\n\nclass ChipTestRAG:\n    def __init__(self, api_key: str, model_name: str = \"gemini-pro\"):\n        \"\"\"初始化RAG系统\n        \n        Args:\n            api_key: Gemini API密钥\n            model_name: 使用的Gemini模型名称\n        \"\"\"\n        self.api_key = api_key\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel(model_name)\n        \n        # 初始化文本分割器\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            length_function=len,\n        )\n        \n        # 初始化嵌入模型\n        self.embeddings = HuggingFaceEmbeddings(\n            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n        )\n        \n        self.vector_store = None\n\n    def load_documents(self, data_dir: str):\n        \"\"\"加载测试用例文档\n        \n        Args:\n            data_dir: 包含测试用例和C代码的目录路径\n        \"\"\"\n        # 加载.c和.txt文件\n        loader = DirectoryLoader(data_dir, glob=\"**/*.[ct]*\")\n        documents = loader.load()\n        \n        # 分割文档\n        texts = self.text_splitter.split_documents(documents)\n        \n        # 创建向量存储\n        self.vector_store = FAISS.from_documents(texts, self.embeddings)\n        \n        return len(texts)\n\n    def query(self, question: str, k: int = 3) -> str:\n        \"\"\"查询系统\n        \n        Args:\n            question: 用户问题\n            k: 返回的相关文档数量\n            \n        Returns:\n            str: Gemini生成的回答\n        \"\"\"\n        if not self.vector_store:\n            raise ValueError(\"请先加载文档!\")\n            \n        # 检索相关文档\n        docs = self.vector_store.similarity_search(question, k=k)\n        context = \"\\n\".join(doc.page_content for doc in docs)\n        \n        # 构建提示\n        prompt = f\"\"\"基于以下芯片测试用例和代码的上下文来回答问题:\n        \n上下文:\n{context}\n\n问题: {question}\n\n请提供详细的解答,并在适当时引用相关的代码片段。\n\"\"\"\n        \n        # 调用Gemini API生成回答\n        response = self.model.generate_content(prompt)\n        return response.text\n\n    def save_vector_store(self, path: str):\n        \"\"\"保存向量存储到磁盘\n        \n        Args:\n            path: 保存路径\n        \"\"\"\n        if self.vector_store:\n            self.vector_store.save_local(path)\n\n    def load_vector_store(self, path: str):\n        \"\"\"从磁盘加载向量存储\n        \n        Args:\n            path: 向量存储路径\n        \"\"\"\n        if os.path.exists(path):\n            self.vector_store = FAISS.load_local(path, self.embeddings)\n\n\nrag = ChipTestRAG(\"xxx\")\nrag.load_documents(\"/kaggle/input/chiptest\")\nrag.save_vector_store(\"vector_store\")\nanswer = rag.query(\"ADC采样测试\")\nprint(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}